{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b436218",
   "metadata": {
    "papermill": {
     "duration": 0.007474,
     "end_time": "2022-06-28T16:57:56.850214",
     "exception": false,
     "start_time": "2022-06-28T16:57:56.842740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# Missing Data\n",
    "\n",
    "Often data sources are incomplete, which means you will have missing data, you have 3 basic options for filling in missing data (you will personally have to make the decision for what is the right approach:\n",
    "\n",
    "* Just keep the missing data points.\n",
    "* Drop them missing data points (including the entire row)\n",
    "* Fill them in with some other value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db644d25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T16:57:56.866626Z",
     "iopub.status.busy": "2022-06-28T16:57:56.865451Z",
     "iopub.status.idle": "2022-06-28T16:59:01.984771Z",
     "shell.execute_reply": "2022-06-28T16:59:01.983259Z"
    },
    "papermill": {
     "duration": 65.166862,
     "end_time": "2022-06-28T16:59:02.023965",
     "exception": false,
     "start_time": "2022-06-28T16:57:56.857103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.5 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9.5)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=a081e844971363eed5ec56e1c62e881f1a875c8323f4e2c11e905544cdb61b57\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: pyspark\r\n",
      "Successfully installed pyspark-3.3.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db160b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:02.101100Z",
     "iopub.status.busy": "2022-06-28T16:59:02.100669Z",
     "iopub.status.idle": "2022-06-28T16:59:07.897715Z",
     "shell.execute_reply": "2022-06-28T16:59:07.896644Z"
    },
    "papermill": {
     "duration": 5.838643,
     "end_time": "2022-06-28T16:59:07.900400",
     "exception": false,
     "start_time": "2022-06-28T16:59:02.061757",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/28 16:59:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"missingdata\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39e78128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:07.978737Z",
     "iopub.status.busy": "2022-06-28T16:59:07.977868Z",
     "iopub.status.idle": "2022-06-28T16:59:14.562303Z",
     "shell.execute_reply": "2022-06-28T16:59:14.560198Z"
    },
    "papermill": {
     "duration": 6.631195,
     "end_time": "2022-06-28T16:59:14.568790",
     "exception": false,
     "start_time": "2022-06-28T16:59:07.937595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"../input/containsnull/ContainsNull.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "915aa93d",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:14.674122Z",
     "iopub.status.busy": "2022-06-28T16:59:14.673730Z",
     "iopub.status.idle": "2022-06-28T16:59:15.021858Z",
     "shell.execute_reply": "2022-06-28T16:59:15.019721Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.398201,
     "end_time": "2022-06-28T16:59:15.025727",
     "exception": false,
     "start_time": "2022-06-28T16:59:14.627526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8279acf2",
   "metadata": {
    "papermill": {
     "duration": 0.036914,
     "end_time": "2022-06-28T16:59:15.100401",
     "exception": false,
     "start_time": "2022-06-28T16:59:15.063487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Drop the missing data\n",
    "\n",
    "You can use the .na functions for missing data. The drop command has the following parameters:\n",
    "\n",
    "    df.na.drop(how='any', thresh=None, subset=None)\n",
    "    \n",
    "    * param how: 'any' or 'all'.\n",
    "    \n",
    "        If 'any', drop a row if it contains any nulls.\n",
    "        If 'all', drop a row only if all its values are null.\n",
    "    \n",
    "    * param thresh: int, default None\n",
    "    \n",
    "        If specified, drop rows that have less than `thresh` non-null values.\n",
    "        This overwrites the `how` parameter.\n",
    "        \n",
    "    * param subset: \n",
    "        optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6f0cd0b",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:15.176135Z",
     "iopub.status.busy": "2022-06-28T16:59:15.175729Z",
     "iopub.status.idle": "2022-06-28T16:59:15.430133Z",
     "shell.execute_reply": "2022-06-28T16:59:15.429028Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.298263,
     "end_time": "2022-06-28T16:59:15.435591",
     "exception": false,
     "start_time": "2022-06-28T16:59:15.137328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop any row that contains missing data\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4806301",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:15.523148Z",
     "iopub.status.busy": "2022-06-28T16:59:15.522385Z",
     "iopub.status.idle": "2022-06-28T16:59:15.742480Z",
     "shell.execute_reply": "2022-06-28T16:59:15.741323Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.262421,
     "end_time": "2022-06-28T16:59:15.745885",
     "exception": false,
     "start_time": "2022-06-28T16:59:15.483464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Has to have at least 2 NON-null values\n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29bdb389",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:15.832273Z",
     "iopub.status.busy": "2022-06-28T16:59:15.831868Z",
     "iopub.status.idle": "2022-06-28T16:59:16.033301Z",
     "shell.execute_reply": "2022-06-28T16:59:16.031982Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.24299,
     "end_time": "2022-06-28T16:59:16.035653",
     "exception": false,
     "start_time": "2022-06-28T16:59:15.792663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbf1a807",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:16.133673Z",
     "iopub.status.busy": "2022-06-28T16:59:16.133164Z",
     "iopub.status.idle": "2022-06-28T16:59:16.328910Z",
     "shell.execute_reply": "2022-06-28T16:59:16.327673Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.239261,
     "end_time": "2022-06-28T16:59:16.332284",
     "exception": false,
     "start_time": "2022-06-28T16:59:16.093023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9be27809",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:16.453045Z",
     "iopub.status.busy": "2022-06-28T16:59:16.452385Z",
     "iopub.status.idle": "2022-06-28T16:59:16.657102Z",
     "shell.execute_reply": "2022-06-28T16:59:16.654991Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.267737,
     "end_time": "2022-06-28T16:59:16.659781",
     "exception": false,
     "start_time": "2022-06-28T16:59:16.392044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec018a",
   "metadata": {
    "papermill": {
     "duration": 0.038702,
     "end_time": "2022-06-28T16:59:16.737341",
     "exception": false,
     "start_time": "2022-06-28T16:59:16.698639",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fill the missing values\n",
    "\n",
    "We can also fill the missing values with new values. If you have multiple nulls across multiple data types, Spark is actually smart enough to match up the data types. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3516fd9",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:16.816757Z",
     "iopub.status.busy": "2022-06-28T16:59:16.816251Z",
     "iopub.status.idle": "2022-06-28T16:59:17.041144Z",
     "shell.execute_reply": "2022-06-28T16:59:17.039988Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.270701,
     "end_time": "2022-06-28T16:59:17.047008",
     "exception": false,
     "start_time": "2022-06-28T16:59:16.776307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+\n",
      "|  Id|      Name|Sales|\n",
      "+----+----------+-----+\n",
      "|emp1|      John| null|\n",
      "|emp2|TEMP VALUE| null|\n",
      "|emp3|TEMP VALUE|345.0|\n",
      "|emp4|     Cindy|456.0|\n",
      "+----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('TEMP VALUE').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8e7026c",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:17.134108Z",
     "iopub.status.busy": "2022-06-28T16:59:17.133597Z",
     "iopub.status.idle": "2022-06-28T16:59:17.456676Z",
     "shell.execute_reply": "2022-06-28T16:59:17.454558Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.368425,
     "end_time": "2022-06-28T16:59:17.461613",
     "exception": false,
     "start_time": "2022-06-28T16:59:17.093188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  0.0|\n",
      "|emp2| null|  0.0|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3f268",
   "metadata": {
    "papermill": {
     "duration": 0.05329,
     "end_time": "2022-06-28T16:59:17.573527",
     "exception": false,
     "start_time": "2022-06-28T16:59:17.520237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Usually you should specify what columns you want to fill with the subset parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea55a54c",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:17.652084Z",
     "iopub.status.busy": "2022-06-28T16:59:17.651616Z",
     "iopub.status.idle": "2022-06-28T16:59:17.837417Z",
     "shell.execute_reply": "2022-06-28T16:59:17.836277Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.231506,
     "end_time": "2022-06-28T16:59:17.843140",
     "exception": false,
     "start_time": "2022-06-28T16:59:17.611634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2|NoONE| null|\n",
      "|emp3|NoONE|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill('NoONE',subset=['Name']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1442d2",
   "metadata": {
    "papermill": {
     "duration": 0.043665,
     "end_time": "2022-06-28T16:59:17.952672",
     "exception": false,
     "start_time": "2022-06-28T16:59:17.909007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "A very common practice is to fill values with the mean value for the column, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1da4ce8c",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:18.031133Z",
     "iopub.status.busy": "2022-06-28T16:59:18.030716Z",
     "iopub.status.idle": "2022-06-28T16:59:18.714570Z",
     "shell.execute_reply": "2022-06-28T16:59:18.713244Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.725984,
     "end_time": "2022-06-28T16:59:18.717230",
     "exception": false,
     "start_time": "2022-06-28T16:59:17.991246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "mean_val = df.select(mean(df['Sales'])).collect()\n",
    "\n",
    "# Weird nested formatting of Row object!\n",
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ded4c1df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:18.799726Z",
     "iopub.status.busy": "2022-06-28T16:59:18.798956Z",
     "iopub.status.idle": "2022-06-28T16:59:18.803745Z",
     "shell.execute_reply": "2022-06-28T16:59:18.802940Z"
    },
    "papermill": {
     "duration": 0.047431,
     "end_time": "2022-06-28T16:59:18.805515",
     "exception": false,
     "start_time": "2022-06-28T16:59:18.758084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_sales = mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8864d3bc",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:18.885837Z",
     "iopub.status.busy": "2022-06-28T16:59:18.885101Z",
     "iopub.status.idle": "2022-06-28T16:59:19.031960Z",
     "shell.execute_reply": "2022-06-28T16:59:19.030302Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.191395,
     "end_time": "2022-06-28T16:59:19.036315",
     "exception": false,
     "start_time": "2022-06-28T16:59:18.844920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(mean_sales,[\"Sales\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47023458",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-28T16:59:19.115823Z",
     "iopub.status.busy": "2022-06-28T16:59:19.115453Z",
     "iopub.status.idle": "2022-06-28T16:59:19.447176Z",
     "shell.execute_reply": "2022-06-28T16:59:19.445990Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.376737,
     "end_time": "2022-06-28T16:59:19.452262",
     "exception": false,
     "start_time": "2022-06-28T16:59:19.075525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# One (very ugly) one-liner\n",
    "df.na.fill(df.select(mean(df['Sales'])).collect()[0][0],['Sales']).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 94.887276,
   "end_time": "2022-06-28T16:59:22.135682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-28T16:57:47.248406",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
